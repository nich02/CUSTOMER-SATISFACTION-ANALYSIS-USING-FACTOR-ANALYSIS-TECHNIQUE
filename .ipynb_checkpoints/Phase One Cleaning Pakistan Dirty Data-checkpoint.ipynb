{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is Pakistan data which contains 493 entries. Our objective in this notebook is to perform general data cleaning to prepare the data for analysis, visualization and modelling. \n",
    "\n",
    "Therefore, this notebook entails data cleaning processess, and techniques for categorial and quantitative columns. \n",
    "\n",
    "The end goal, to make the data clean for anyone interested in performing any type of processing to move on...Also, there will be removal of some columns we might not need. \n",
    "\n",
    "\n",
    "Following stages will focus on:\n",
    "\n",
    "2. Data analysis and Visualization \n",
    "3. Predictive Modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "working as a demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase One: Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing dataset, and important libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date</th>\n",
       "      <th>Islamic Date</th>\n",
       "      <th>Blast Day Type</th>\n",
       "      <th>Holiday Type</th>\n",
       "      <th>Time</th>\n",
       "      <th>City</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Province</th>\n",
       "      <th>...</th>\n",
       "      <th>Targeted Sect if any</th>\n",
       "      <th>Killed Min</th>\n",
       "      <th>Killed Max</th>\n",
       "      <th>Injured Min</th>\n",
       "      <th>Injured Max</th>\n",
       "      <th>No. of Suicide Blasts</th>\n",
       "      <th>Explosive Weight (max)</th>\n",
       "      <th>Hospital Names</th>\n",
       "      <th>Temperature(C)</th>\n",
       "      <th>Temperature(F)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Sunday-November 19-1995</td>\n",
       "      <td>25 Jumaada al-THaany 1416 A.H</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>Weekend</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Islamabad</td>\n",
       "      <td>33.7180</td>\n",
       "      <td>73.0718</td>\n",
       "      <td>Capital</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.835</td>\n",
       "      <td>60.503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Monday-November 6-2000</td>\n",
       "      <td>10 SHa`baan 1421 A.H</td>\n",
       "      <td>Working Day</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Karachi</td>\n",
       "      <td>24.9918</td>\n",
       "      <td>66.9911</td>\n",
       "      <td>Sindh</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.770</td>\n",
       "      <td>74.786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Wednesday-May 8-2002</td>\n",
       "      <td>25 safar 1423 A.H</td>\n",
       "      <td>Working Day</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7:45 AM</td>\n",
       "      <td>Karachi</td>\n",
       "      <td>24.9918</td>\n",
       "      <td>66.9911</td>\n",
       "      <td>Sindh</td>\n",
       "      <td>...</td>\n",
       "      <td>Christian</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5 Kg</td>\n",
       "      <td>1.Jinnah Postgraduate Medical Center 2. Civil ...</td>\n",
       "      <td>31.460</td>\n",
       "      <td>88.628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Friday-June 14-2002</td>\n",
       "      <td>3 Raby` al-THaany 1423 A.H</td>\n",
       "      <td>Working Day</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11:10:00 AM</td>\n",
       "      <td>Karachi</td>\n",
       "      <td>24.9918</td>\n",
       "      <td>66.9911</td>\n",
       "      <td>Sindh</td>\n",
       "      <td>...</td>\n",
       "      <td>Christian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.430</td>\n",
       "      <td>88.574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Friday-July 4-2003</td>\n",
       "      <td>4 Jumaada al-awal 1424 A.H</td>\n",
       "      <td>Working Day</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Quetta</td>\n",
       "      <td>30.2095</td>\n",
       "      <td>67.0182</td>\n",
       "      <td>Baluchistan</td>\n",
       "      <td>...</td>\n",
       "      <td>Shiite</td>\n",
       "      <td>44.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.CMH Quetta \\n2.Civil Hospital 3. Boland Medi...</td>\n",
       "      <td>33.120</td>\n",
       "      <td>91.616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                     Date                   Islamic Date  \\\n",
       "0           1  Sunday-November 19-1995  25 Jumaada al-THaany 1416 A.H   \n",
       "1           2   Monday-November 6-2000           10 SHa`baan 1421 A.H   \n",
       "2           3     Wednesday-May 8-2002              25 safar 1423 A.H   \n",
       "3           4      Friday-June 14-2002     3 Raby` al-THaany 1423 A.H   \n",
       "4           5       Friday-July 4-2003     4 Jumaada al-awal 1424 A.H   \n",
       "\n",
       "  Blast Day Type Holiday Type         Time       City  Latitude Longitude  \\\n",
       "0        Holiday      Weekend          NaN  Islamabad   33.7180   73.0718   \n",
       "1    Working Day          NaN          NaN    Karachi   24.9918   66.9911   \n",
       "2    Working Day          NaN      7:45 AM   Karachi    24.9918   66.9911   \n",
       "3    Working Day          NaN  11:10:00 AM    Karachi   24.9918   66.9911   \n",
       "4    Working Day          NaN          NaN     Quetta   30.2095   67.0182   \n",
       "\n",
       "      Province  ... Targeted Sect if any Killed Min Killed Max Injured Min  \\\n",
       "0      Capital  ...                 None       14.0       15.0         NaN   \n",
       "1        Sindh  ...                 None        NaN        3.0         NaN   \n",
       "2        Sindh  ...            Christian       13.0       15.0        20.0   \n",
       "3        Sindh  ...            Christian        NaN       12.0         NaN   \n",
       "4  Baluchistan  ...               Shiite       44.0       47.0         NaN   \n",
       "\n",
       "  Injured Max No. of Suicide Blasts Explosive Weight (max)  \\\n",
       "0          60                   2.0                    NaN   \n",
       "1           3                   1.0                    NaN   \n",
       "2          40                   1.0                 2.5 Kg   \n",
       "3          51                   1.0                    NaN   \n",
       "4          65                   1.0                    NaN   \n",
       "\n",
       "                                      Hospital Names  Temperature(C)  \\\n",
       "0                                                NaN          15.835   \n",
       "1                                                NaN          23.770   \n",
       "2  1.Jinnah Postgraduate Medical Center 2. Civil ...          31.460   \n",
       "3                                                NaN          31.430   \n",
       "4  1.CMH Quetta \\n2.Civil Hospital 3. Boland Medi...          33.120   \n",
       "\n",
       "   Temperature(F)  \n",
       "0          60.503  \n",
       "1          74.786  \n",
       "2          88.628  \n",
       "3          88.574  \n",
       "4          91.616  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing relevant libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    " ~\n",
    "\n",
    "During uploading this dataset it failed to uplaod and brought the error that (UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa0 in position 0: invalid start byte)\n",
    "Such an error  means the dataset contains some special characters.\n",
    "You have to use the encoding as latin1 to read this file as there are some special character in this file, use the below code snippet to read the file.\n",
    "''' \n",
    "pakistan=pd.read_csv(\"C:\\\\Users\\\\ADMIN\\\\Desktop\\\\PakistanV6.csv\",encoding='latin1')\n",
    "pakistan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(492, 26)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#What's the shape of our dataset?\n",
    "pakistan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 492 entries, 0 to 491\n",
      "Data columns (total 26 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Unnamed: 0               492 non-null    int64  \n",
      " 1   Date                     492 non-null    object \n",
      " 2   Islamic Date             336 non-null    object \n",
      " 3   Blast Day Type           481 non-null    object \n",
      " 4   Holiday Type             72 non-null     object \n",
      " 5   Time                     281 non-null    object \n",
      " 6   City                     492 non-null    object \n",
      " 7   Latitude                 490 non-null    float64\n",
      " 8   Longitude                490 non-null    object \n",
      " 9   Province                 492 non-null    object \n",
      " 10  Location                 489 non-null    object \n",
      " 11  Location Category        457 non-null    object \n",
      " 12  Location Sensitivity     456 non-null    object \n",
      " 13  Open/Closed Space        457 non-null    object \n",
      " 14  Influencing Event/Event  187 non-null    object \n",
      " 15  Target Type              466 non-null    object \n",
      " 16  Targeted Sect if any     443 non-null    object \n",
      " 17  Killed Min               346 non-null    float64\n",
      " 18  Killed Max               476 non-null    float64\n",
      " 19  Injured Min              361 non-null    float64\n",
      " 20  Injured Max              459 non-null    object \n",
      " 21  No. of Suicide Blasts    410 non-null    float64\n",
      " 22  Explosive Weight (max)   169 non-null    object \n",
      " 23  Hospital Names           294 non-null    object \n",
      " 24  Temperature(C)           487 non-null    float64\n",
      " 25  Temperature(F)           485 non-null    float64\n",
      "dtypes: float64(7), int64(1), object(18)\n",
      "memory usage: 100.1+ KB\n"
     ]
    }
   ],
   "source": [
    "#Let's get a glimps of our dataset..\n",
    "pakistan.info();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output above we discover that, there exist some null values and our dataset contains both float and object datatypes. There're some things to deal with in due course..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Date', 'Islamic Date', 'Blast Day Type', 'Holiday Type',\n",
       "       'Time', 'City', 'Latitude', 'Longitude', 'Province', 'Location',\n",
       "       'Location Category', 'Location Sensitivity', 'Open/Closed Space',\n",
       "       'Influencing Event/Event', 'Target Type', 'Targeted Sect if any',\n",
       "       'Killed Min', 'Killed Max', 'Injured Min', 'Injured Max',\n",
       "       'No. of Suicide Blasts', 'Explosive Weight (max)', 'Hospital Names',\n",
       "       'Temperature(C)', 'Temperature(F)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Which columns do we have?\n",
    "pakistan.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to drop some columns we might not need in future, we won't need the following columns.\n",
    "    \n",
    "    1. Killed Min\n",
    "    2. Injured Min\n",
    "    3. Location Category\n",
    "    4. Open/Closed Space\n",
    "    3. Explosive Weight (max)\n",
    "So before we start cleaning our data we need to drop the unwanted columns to clean what we will use to avoid wasting time unnecessariry. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date</th>\n",
       "      <th>Islamic Date</th>\n",
       "      <th>Blast Day Type</th>\n",
       "      <th>Holiday Type</th>\n",
       "      <th>Time</th>\n",
       "      <th>City</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Province</th>\n",
       "      <th>...</th>\n",
       "      <th>Location Category</th>\n",
       "      <th>Location Sensitivity</th>\n",
       "      <th>Influencing Event/Event</th>\n",
       "      <th>Target Type</th>\n",
       "      <th>Targeted Sect if any</th>\n",
       "      <th>Killed Max</th>\n",
       "      <th>Injured Max</th>\n",
       "      <th>No. of Suicide Blasts</th>\n",
       "      <th>Hospital Names</th>\n",
       "      <th>Temperature(C)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Sunday-November 19-1995</td>\n",
       "      <td>25 Jumaada al-THaany 1416 A.H</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>Weekend</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Islamabad</td>\n",
       "      <td>33.7180</td>\n",
       "      <td>73.0718</td>\n",
       "      <td>Capital</td>\n",
       "      <td>...</td>\n",
       "      <td>Foreign</td>\n",
       "      <td>High</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Foreigner</td>\n",
       "      <td>None</td>\n",
       "      <td>15.0</td>\n",
       "      <td>60</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Monday-November 6-2000</td>\n",
       "      <td>10 SHa`baan 1421 A.H</td>\n",
       "      <td>Working Day</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Karachi</td>\n",
       "      <td>24.9918</td>\n",
       "      <td>66.9911</td>\n",
       "      <td>Sindh</td>\n",
       "      <td>...</td>\n",
       "      <td>Office Building</td>\n",
       "      <td>Low</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Media</td>\n",
       "      <td>None</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Wednesday-May 8-2002</td>\n",
       "      <td>25 safar 1423 A.H</td>\n",
       "      <td>Working Day</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7:45 AM</td>\n",
       "      <td>Karachi</td>\n",
       "      <td>24.9918</td>\n",
       "      <td>66.9911</td>\n",
       "      <td>Sindh</td>\n",
       "      <td>...</td>\n",
       "      <td>Hotel</td>\n",
       "      <td>Medium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Foreigner</td>\n",
       "      <td>Christian</td>\n",
       "      <td>15.0</td>\n",
       "      <td>40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.Jinnah Postgraduate Medical Center 2. Civil ...</td>\n",
       "      <td>31.460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Friday-June 14-2002</td>\n",
       "      <td>3 Raby` al-THaany 1423 A.H</td>\n",
       "      <td>Working Day</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11:10:00 AM</td>\n",
       "      <td>Karachi</td>\n",
       "      <td>24.9918</td>\n",
       "      <td>66.9911</td>\n",
       "      <td>Sindh</td>\n",
       "      <td>...</td>\n",
       "      <td>Foreign</td>\n",
       "      <td>High</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Foreigner</td>\n",
       "      <td>Christian</td>\n",
       "      <td>12.0</td>\n",
       "      <td>51</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Friday-July 4-2003</td>\n",
       "      <td>4 Jumaada al-awal 1424 A.H</td>\n",
       "      <td>Working Day</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Quetta</td>\n",
       "      <td>30.2095</td>\n",
       "      <td>67.0182</td>\n",
       "      <td>Baluchistan</td>\n",
       "      <td>...</td>\n",
       "      <td>Religious</td>\n",
       "      <td>Medium</td>\n",
       "      <td>during Friday prayer</td>\n",
       "      <td>Religious</td>\n",
       "      <td>Shiite</td>\n",
       "      <td>47.0</td>\n",
       "      <td>65</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.CMH Quetta \\n2.Civil Hospital 3. Boland Medi...</td>\n",
       "      <td>33.120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                     Date                   Islamic Date  \\\n",
       "0           1  Sunday-November 19-1995  25 Jumaada al-THaany 1416 A.H   \n",
       "1           2   Monday-November 6-2000           10 SHa`baan 1421 A.H   \n",
       "2           3     Wednesday-May 8-2002              25 safar 1423 A.H   \n",
       "3           4      Friday-June 14-2002     3 Raby` al-THaany 1423 A.H   \n",
       "4           5       Friday-July 4-2003     4 Jumaada al-awal 1424 A.H   \n",
       "\n",
       "  Blast Day Type Holiday Type         Time       City  Latitude Longitude  \\\n",
       "0        Holiday      Weekend          NaN  Islamabad   33.7180   73.0718   \n",
       "1    Working Day          NaN          NaN    Karachi   24.9918   66.9911   \n",
       "2    Working Day          NaN      7:45 AM   Karachi    24.9918   66.9911   \n",
       "3    Working Day          NaN  11:10:00 AM    Karachi   24.9918   66.9911   \n",
       "4    Working Day          NaN          NaN     Quetta   30.2095   67.0182   \n",
       "\n",
       "      Province  ... Location Category Location Sensitivity  \\\n",
       "0      Capital  ...           Foreign                 High   \n",
       "1        Sindh  ...   Office Building                  Low   \n",
       "2        Sindh  ...             Hotel               Medium   \n",
       "3        Sindh  ...           Foreign                 High   \n",
       "4  Baluchistan  ...         Religious               Medium   \n",
       "\n",
       "  Influencing Event/Event Target Type Targeted Sect if any Killed Max  \\\n",
       "0                     NaN   Foreigner                 None       15.0   \n",
       "1                     NaN       Media                 None        3.0   \n",
       "2                     NaN   Foreigner            Christian       15.0   \n",
       "3                     NaN   Foreigner            Christian       12.0   \n",
       "4    during Friday prayer   Religious               Shiite       47.0   \n",
       "\n",
       "   Injured Max No. of Suicide Blasts  \\\n",
       "0           60                   2.0   \n",
       "1            3                   1.0   \n",
       "2           40                   1.0   \n",
       "3           51                   1.0   \n",
       "4           65                   1.0   \n",
       "\n",
       "                                      Hospital Names Temperature(C)  \n",
       "0                                                NaN         15.835  \n",
       "1                                                NaN         23.770  \n",
       "2  1.Jinnah Postgraduate Medical Center 2. Civil ...         31.460  \n",
       "3                                                NaN         31.430  \n",
       "4  1.CMH Quetta \\n2.Civil Hospital 3. Boland Medi...         33.120  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#So here-below we drop these unwanted columns before going further..\n",
    "def keep_cols(DataFrame, keep_these):\n",
    "    \"\"\"Keep only the columns [keep_these] in a DataFrame, delete\n",
    "    all other columns. \n",
    "    \"\"\"\n",
    "    drop_these = list(set(list(DataFrame)) - set(keep_these))\n",
    "    return DataFrame.drop(drop_these, axis = 1)\n",
    "\n",
    "new_data = pakistan.pipe(keep_cols, ['Unnamed: 0', 'Date', 'Islamic Date', 'Blast Day Type', 'Holiday Type',\n",
    "       'Time', 'City', 'Latitude', 'Longitude', 'Province', 'Location',\n",
    "       'Location Category', 'Location Sensitivity', 'Influencing Event/Event', 'Target Type', 'Targeted Sect if any',\n",
    "       'Killed Max', 'Injured Max',\n",
    "       'No. of Suicide Blasts', 'Hospital Names',\n",
    "       'Temperature(C)'])\n",
    "new_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can discover that the above function has worked because now we've 22 columns instead of the initial 26. That means our function has worked out effectivelly. \n",
    "Now we can continue to perform cleaning on columns we can use only "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following steps deal with quantitative values filling, or dropping the nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(492, 21)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: (0, 21)\n"
     ]
    }
   ],
   "source": [
    "#Let's see if we have duplicate rows in our dataset and deal with them\n",
    "duplicate_rows = new_data[new_data.duplicated()]\n",
    "print(\"Number of duplicate rows:\", duplicate_rows.shape)\n",
    "\n",
    "#or write this function, #duplicate_rows.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output it shows, there're no duplicate rows in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column Name</th>\n",
       "      <th>Missing Values Count</th>\n",
       "      <th>Filling Factor (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Holiday Type</td>\n",
       "      <td>420</td>\n",
       "      <td>14.634146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Influencing Event/Event</td>\n",
       "      <td>305</td>\n",
       "      <td>38.008130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Time</td>\n",
       "      <td>211</td>\n",
       "      <td>57.113821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hospital Names</td>\n",
       "      <td>198</td>\n",
       "      <td>59.756098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Islamic Date</td>\n",
       "      <td>156</td>\n",
       "      <td>68.292683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>No. of Suicide Blasts</td>\n",
       "      <td>82</td>\n",
       "      <td>83.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Targeted Sect if any</td>\n",
       "      <td>49</td>\n",
       "      <td>90.040650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Location Sensitivity</td>\n",
       "      <td>36</td>\n",
       "      <td>92.682927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Location Category</td>\n",
       "      <td>35</td>\n",
       "      <td>92.886179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Injured Max</td>\n",
       "      <td>33</td>\n",
       "      <td>93.292683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Target Type</td>\n",
       "      <td>26</td>\n",
       "      <td>94.715447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Killed Max</td>\n",
       "      <td>16</td>\n",
       "      <td>96.747967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Blast Day Type</td>\n",
       "      <td>11</td>\n",
       "      <td>97.764228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Temperature(C)</td>\n",
       "      <td>5</td>\n",
       "      <td>98.983740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Location</td>\n",
       "      <td>3</td>\n",
       "      <td>99.390244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Longitude</td>\n",
       "      <td>2</td>\n",
       "      <td>99.593496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Latitude</td>\n",
       "      <td>2</td>\n",
       "      <td>99.593496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>City</td>\n",
       "      <td>0</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Date</td>\n",
       "      <td>0</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Province</td>\n",
       "      <td>0</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Unnamed: 0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Column Name  Missing Values Count  Filling Factor (%)\n",
       "0              Holiday Type                   420           14.634146\n",
       "1   Influencing Event/Event                   305           38.008130\n",
       "2                      Time                   211           57.113821\n",
       "3            Hospital Names                   198           59.756098\n",
       "4              Islamic Date                   156           68.292683\n",
       "5     No. of Suicide Blasts                    82           83.333333\n",
       "6      Targeted Sect if any                    49           90.040650\n",
       "7      Location Sensitivity                    36           92.682927\n",
       "8         Location Category                    35           92.886179\n",
       "9               Injured Max                    33           93.292683\n",
       "10              Target Type                    26           94.715447\n",
       "11               Killed Max                    16           96.747967\n",
       "12           Blast Day Type                    11           97.764228\n",
       "13           Temperature(C)                     5           98.983740\n",
       "14                 Location                     3           99.390244\n",
       "15                Longitude                     2           99.593496\n",
       "16                 Latitude                     2           99.593496\n",
       "17                     City                     0          100.000000\n",
       "18                     Date                     0          100.000000\n",
       "19                 Province                     0          100.000000\n",
       "20               Unnamed: 0                     0          100.000000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets see if we have null values in our dataset\\\n",
    "\n",
    "#One function to check nulls is here  new_data.isna().sum()\n",
    "\n",
    "# Checking the Missing Values in the remaining columns in the dataset and the percentage to fill\n",
    "missing_new_data =new_data.isnull().sum(axis=0).reset_index()\n",
    "missing_new_data.columns = ['Column Name', 'Missing Values Count']\n",
    "missing_new_data['Filling Factor (%)']=(new_data.shape[0]-missing_new_data['Missing Values Count'])/new_data.shape[0]*100\n",
    "missing_new_data.sort_values('Filling Factor (%)').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output shows alot of missing values, to proceed we have to deal with quantitative values first\n",
    "Time, Islamic Date, No. of Suicide Blasts, Injured Max, Killed Max, Temperature(C) Longitude & Latitude, Islamic Date, Date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# i. Tidining up the quantitative columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following columns, Temperature(C), Killed Max, Injured Max, & No. of Suicide Blasts, I decide the filling creteria to be mean.\n",
    "\n",
    "The following cells show the filling process\n",
    "\n",
    "\n",
    "Learning process\n",
    "\n",
    "Now I want to fill 3 columns with mode value, then fill the 3 columns with mean value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the mode.. of our columns\n",
    "mode = new_data.filter([\"Temperature(C)\", \"Killed Max\", \"Injured Max\"]).mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we want to fill our 3 columns with mode value of each column using this function\n",
    "cols = [\"Temperature(C)\", \"Killed Max\", \"Injured Max\"]\n",
    "\n",
    "new_data[cols]=new_data[cols].fillna(new_data.mode().iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fill the same columns with mean value of each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want  a function to fill multiple columns with mean value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling with mean value of 21\n"
     ]
    }
   ],
   "source": [
    "#We deal with these columns first (No. of Suicide Blasts, Injured Max, Killed Max, Temperature(C) )\n",
    "\n",
    "mean_new_data = round(new_data['Temperature(C)'].mean())\n",
    "\n",
    "#Fill with mean\n",
    "print(\"Filling with mean value of {}\".format(mean_new_data))\n",
    "new_data['Temperature(C)'] = new_data['Temperature(C)'].fillna(mean_new_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling with mean value of 15\n"
     ]
    }
   ],
   "source": [
    "#We deal with these columns first (No. of Suicide Blasts, Injured Max, Killed Max, Temperature(C) )\n",
    "\n",
    "mean_new_data = round(new_data['Killed Max'].mean())\n",
    "\n",
    "#Fill with mean\n",
    "print(\"Filling with mean value of {}\".format(mean_new_data))\n",
    "new_data['Killed Max'] = new_data['Killed Max'].fillna(mean_new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"int\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-e33ec883e3ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#We deal with these columns first (No. of Suicide Blasts, Injured Max, Killed Max, Temperature(C) )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmean_new_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Injured Max'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#Fill with mean\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mstat_func\u001b[1;34m(self, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  11215\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_agg_by_level\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11216\u001b[0m         return self._reduce(\n\u001b[1;32m> 11217\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  11218\u001b[0m         )\n\u001b[0;32m  11219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_reduce\u001b[1;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[0;32m   3889\u001b[0m                 )\n\u001b[0;32m   3890\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3891\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3893\u001b[0m         \u001b[1;31m# TODO(EA) dispatch to Index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\nanops.py\u001b[0m in \u001b[0;36m_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;31m# we want to transform an object array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\nanops.py\u001b[0m in \u001b[0;36mf\u001b[1;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[0;32m    123\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\nanops.py\u001b[0m in \u001b[0;36mnanmean\u001b[1;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[0mdtype_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m     \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m     \u001b[0mthe_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ensure_numeric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype_sum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthe_sum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ndim\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     36\u001b[0m def _sum(a, axis=None, dtype=None, out=None, keepdims=False,\n\u001b[0;32m     37\u001b[0m          initial=_NoValue, where=True):\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m def _prod(a, axis=None, dtype=None, out=None, keepdims=False,\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"int\") to str"
     ]
    }
   ],
   "source": [
    "#We deal with these columns first (No. of Suicide Blasts, Injured Max, Killed Max, Temperature(C) )\n",
    "\n",
    "mean_new_data = round(new_data['Injured Max'].mean())\n",
    "\n",
    "#Fill with mean\n",
    "print(\"Filling with mean value of {}\".format(mean_new_data))\n",
    "new_data['Injured Max'] = new_data['Injured Max'].fillna(mean_new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling with mean value of 1\n"
     ]
    }
   ],
   "source": [
    "#We deal with these columns first (No. of Suicide Blasts, Injured Max, Killed Max, Temperature(C) )\n",
    "\n",
    "mean_new_data = round(new_data['No. of Suicide Blasts'].mean())\n",
    "\n",
    "#Fill with mean\n",
    "print(\"Filling with mean value of {}\".format(mean_new_data))\n",
    "new_data['No. of Suicide Blasts'] = new_data['No. of Suicide Blasts'].fillna(mean_new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now dealing with Latitude and Longitude "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latitudes and Longitudes are Geo-based features that contain records about the geography location of a place/point. Features like Longitudes, Latitudes and Address are Geo-features that need to engineered.\n",
    "\n",
    "We can use libraries like Geojson, Geopy to convert these numerical values to physical address on a map. But this method is slow and does not really scale to large number of features. In this article, we will bypass this method and show simpler and quicker ways to extract features from longituds and latitudes. \n",
    "\n",
    "The manhattan distance is the sum of the horizontal and vertical distance between two points. Let's demonstrate this below using the sendy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/risenW/Practical_feature_engineering_guide/blob/master/Practical%20Featture%20Engineering%20Guide.ipynb\n",
    "\n",
    "#Manhattan distnace\n",
    "def manhattan_distance(lat1, lng1):\n",
    "    a = np.abs(lat1 -lat1)\n",
    "    #b = np.abs(lng1 - lng2)\n",
    "    #return a + b\n",
    "\n",
    "new_data['distance_manhattan'] = manhattan_distance(new_data['Latitude'].values, new_data['Longitude'].values)\n",
    "                                                   \n",
    "new_data.head()\n",
    "\n",
    "\n",
    "\n",
    "#Bearing method to deal with Latitude and Longitude\n",
    "#We are going to use Bearing method...\n",
    "#Bearing: The bearing is the compass direction to travel from a starting point, and must be within the range 0 to 360.\n",
    "\n",
    "#Here, we first write the bearing function, then we use the function to calculate the bearing between Pickup and Destination.\n",
    "\n",
    "'''\n",
    "#Bearing\n",
    "def bearing_array(lat1, lng1, lat2, lng2):\n",
    "    AVG_EARTH_RADIUS = 6371  # in km\n",
    "    lng_delta_rad = np.radians(lng2 - lng1)\n",
    "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n",
    "    y = np.sin(lng_delta_rad) * np.cos(lat2)\n",
    "    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)\n",
    "    return np.degrees(np.arctan2(y, x))\n",
    "\n",
    "sendy_data['bearing'] = bearing_array(sendy_data['Pickup Lat'].values, sendy_data['Pickup Long'].values,\n",
    "                                                   sendy_data['Destination Lat'].values, sendy_data['Destination Long'].values)\n",
    "\n",
    "sendy_data.head()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now dealing with the nulls within Latitude and Longitude columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we fill the missing with mean \n",
    "mean_new_data = round(new_data['Latitude'].mean())\n",
    "\n",
    "#Fill with mean\n",
    "print(\"Filling with mean value of {}\".format(mean_new_data))\n",
    "new_data['Latitude'] = new_data['Latitude'].fillna(mean_new_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, let's fill the missing values with mean..\n",
    "mean_new_data = round(new_data['Longitude'].mean())\n",
    "\n",
    "#Fill with mean\n",
    "print(\"Filling with mean value of {}\".format(mean_new_data))\n",
    "new_data['Longitude'] = new_data['Longitude'].fillna(mean_new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date Time Now\n",
    "Now dealing with data and time column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First convert to pandas datetime format\n",
    "#Then deal with the missing dates in date time feature and fill them \n",
    "new_data['Time'] = pd.to_datetime(new_data['Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ii. Tidining up the categorical columns\n",
    "Now we want to deal with categorical values, tide them and later encode them..but we'll tide one column 'City' the others 'Province', 'Hospital Names' and the others later...\n",
    "\n",
    "\n",
    "We first start with the city column then later we can check on the Provice column\n",
    "\n",
    "\n",
    "I'm interested in cleaning up the \"City\" column first to make sure there's no data entry inconsistencies in it. We could go through and check each row by hand, of course, and hand-correct inconsistencies when we find them. There's a more efficient way to do this though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful tutorial https://www.kaggle.com/rtatman/data-cleaning-challenge-inconsistent-data-entry\n",
    "\n",
    "'''\n",
    "We first need to import helpful libraries to Do some preliminary text pre-processing\n",
    "'''\n",
    "\n",
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import process\n",
    "import chardet\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# get all the unique values in the 'City' column\n",
    "cities = new_data['City'].unique()\n",
    "# sort them alphabetically and then take a closer look\n",
    "cities.sort()\n",
    "cities\n",
    "\n",
    "\n",
    "#Can I have a code for multiples columns ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just looking at this, I can see some problems due to inconsistent data entry: 'Lahore' and 'Lahore ', for example, or 'Lakki Marwat' and 'Lakki marwat'.\n",
    "\n",
    "The first thing I'm going to do is make everything lower case (I can change it back at the end if I like) and remove any white spaces at the beginning and end of cells. Inconsistencies in capitalizations and trailing white spaces are very common in text data and you can fix a good 80% of your text data entry inconsistencies by doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lower case\n",
    "new_data['City'] = new_data['City'].str.lower()\n",
    "# remove trailing white spaces\n",
    "new_data['City'] = new_data['City'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our next step is to Use fuzzy matching to correct inconsistent data entry\n",
    "\n",
    "let's take another look at the city column and see if there's any more data cleaning we need to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the unique values in the 'City' column, and also Province column.\n",
    "cities = new_data['City'].unique()\n",
    "\n",
    "# sort them alphabetically and then take a closer look\n",
    "cities.sort()\n",
    "cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output above, we can see that all clumns are in small letters, and the inconsistent entries have been removed like  'Lahore' and 'Lahore ', for example, or 'Lakki Marwat' and 'Lakki marwat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does look like there are some remaining inconsistencies: 'd. i khan' and 'd.i khan' should probably be the same. (I looked it up and 'd.g khan' is a seperate city, so I shouldn't combine those.)\n",
    "\n",
    "I'm going to use the fuzzywuzzy package to help identify which string are closest to each other. This dataset is small enough that we could probably could correct errors by hand, but that approach doesn't scale well. (Would you want to correct a thousand errors by hand? What about ten thousand? Automating things as early as possible is generally a good idea. Plus, itâ€™s fun! :)\n",
    "\n",
    "Fuzzy matching: The process of automatically finding text strings that are very similar to the target string. In general, a string is considered \"closer\" to another one the fewer characters you'd need to change if you were transforming one string into another. So \"apple\" and \"snapple\" are two changes away from each other (add \"s\" and \"n\") while \"in\" and \"on\" and one change away (rplace \"i\" with \"o\"). You won't always be able to rely on fuzzy matching 100%, but it will usually end up saving you at least a little time.\n",
    "\n",
    "Fuzzywuzzy returns a ratio given two strings. The closer the ratio is to 100, the smaller the edit distance between the two strings. Here, we're going to get the ten strings from our list of cities that have the closest distance to \"d.i khan\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top 10 closest matches to \"d.i khan\"\n",
    "matches = fuzzywuzzy.process.extract(\"d.i khan\", cities, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
    "\n",
    "# take a look at them\n",
    "matches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output we can see that two of the items in the cities are very close to \"d.i khan\": \"d. i khan\" and \"d.i khan\". We can also see the \"d.g khan\", which is a seperate city, has a ratio of 88. Since we don't want to replace \"d.g khan\" with \"d.i khan\", let's replace all rows in our City column that have a ratio of > 90 with \"d. i khan\".\n",
    "\n",
    "To do this, I'm going to write a function. (It's a good idea to write a general purpose function you can reuse if you think you might have to do a specific task more than once or twice. This keeps you from having to copy and paste code too often, which saves time and can help prevent mistakes.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to replace rows in the provided column of the provided dataframe\n",
    "# that match the provided string above the provided ratio with the provided string\n",
    "def replace_matches_in_column(df, column, string_to_match, min_ratio = 90):\n",
    "    # get a list of unique strings\n",
    "    strings = df[column].unique()\n",
    "    \n",
    "    # get the top 10 closest matches to our input string\n",
    "    matches = fuzzywuzzy.process.extract(string_to_match, strings, \n",
    "                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
    "\n",
    "    # only get matches with a ratio > 90\n",
    "    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]\n",
    "\n",
    "    # get the rows of all the close matches in our dataframe\n",
    "    rows_with_matches = df[column].isin(close_matches)\n",
    "\n",
    "    # replace all rows with close matches with the input matches \n",
    "    df.loc[rows_with_matches, column] = string_to_match\n",
    "    \n",
    "    # let us know the function's done\n",
    "    print(\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's test our function\n",
    "# use the function we just wrote to replace close matches to \"d.i khan\" with \"d.i khan\"\n",
    "replace_matches_in_column(df=suicide_attacks, column='City', string_to_match=\"d.i khan\")\n",
    "\n",
    "\n",
    "# Now we use the function we wrote in the above cell to replace close matches to \"kuram agency\" with \"kuram agency\"\n",
    "             #replace_matches_in_column(df = suicideAttacks, column = 'City', string_to_match = \"kuram agency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's check the unique values in our City column again and make sure we've tidied up d.i khan correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the unique values in the 'Province' column. \n",
    "#Here we expect to output all the columns in the column\n",
    "provinces = new_data['Province'].unique()\n",
    "\n",
    "# sort them alphabetically and then take a closer look\n",
    "provinces.sort()\n",
    "provinces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output, we've some unique columns, We can deal with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the unique values in the 'City' column\n",
    "cities = suicide_attacks['City'].unique()\n",
    "\n",
    "# sort them alphabetically and then take a closer look\n",
    "cities.sort()\n",
    "cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Encoding \n",
    "\n",
    "We are now done with cleaning the 'City' column.. we can do the same to other categorical columns, like 'Province'\n",
    "\n",
    "but first we are going to encode the 'City' column for ML modelling in the immediate cell \n",
    "\n",
    "To perform automated encoding, we will use an efficient library called categorical_encoders. This library offers numerous encoding schemes out of the box and has first hand support for Pandas DataFrame.\n",
    "\n",
    "To install the library, you can use pip as follow:\n",
    "\n",
    "pip install category_encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the \n",
    "\n",
    "Hash Encoding: Hash encoding or feature hashing is a fast and space-efficient way of encoding features. It is very efficient for categorical features with large number of classes. Hash encoder works by applying a hash function to the features.\n",
    "\n",
    "The following code will encode several columns... Lets see it below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'category_encoders'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-ce0f2725445f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#pip install category_encoders\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcategory_encoders\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m cat_cols = ['City', 'Province', 'Location', 'Location Sensitivity', 'Location Category', 'Influencing Event/Event',\n\u001b[0;32m      5\u001b[0m            'Target Type', 'Hospital Names'] \n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'category_encoders'"
     ]
    }
   ],
   "source": [
    "#pip install category_encoders\n",
    "import category_encoders as ce\n",
    "\n",
    "cat_cols = ['City', 'Province', 'Location', 'Location Sensitivity', 'Location Category', 'Influencing Event/Event',\n",
    "           'Target Type', 'Hospital Names'] \n",
    "\n",
    "hash_enc = ce.HashingEncoder(cols=cat_cols, n_components=15)\n",
    "new_data = hash_enc.fit_transform(new_data)\n",
    "\n",
    "new_data.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output above we can see we only have \"d.i khan\" in our dataframe and we didn't have to change anything by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iii. Data Scalling, standardization and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At EDA we apply a function to detect and remove outliers (IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://heartbeat.fritz.ai/exploratory-data-analysis-data-characteristics-and-visualizations-fdf2606ce1ab\n",
    "\n",
    "#Let's see the distribution of the 'Blast Day Type' column, how many Working Days, Holydays, and Weekends\n",
    "new_data['Blast Day Type'].value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
